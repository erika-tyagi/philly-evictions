{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Philly Evictions Analysis\n",
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pipeline\n",
    "\n",
    "pipeline.notebook.set_up()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Our evictions data has already been augmented with data from the ACS and from\n",
    "Philadelphia's open data portal. We load in the final merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEOID</th>\n",
       "      <th>year_evictions</th>\n",
       "      <th>eviction-filings</th>\n",
       "      <th>evictions</th>\n",
       "      <th>low-flag</th>\n",
       "      <th>imputed</th>\n",
       "      <th>subbed</th>\n",
       "      <th>evictions_t-1</th>\n",
       "      <th>evictions_t-2</th>\n",
       "      <th>evictions_t-5</th>\n",
       "      <th>...</th>\n",
       "      <th>units</th>\n",
       "      <th>occupied_units</th>\n",
       "      <th>vacant_units</th>\n",
       "      <th>for_rent_units</th>\n",
       "      <th>num_af_am_alone</th>\n",
       "      <th>num_hisp</th>\n",
       "      <th>black_alone_owner_occupied</th>\n",
       "      <th>num_with_high_school_degree</th>\n",
       "      <th>num_with_ged</th>\n",
       "      <th>num_unemployed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>421010001001</td>\n",
       "      <td>2010</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>...</td>\n",
       "      <td>220.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>421010001001</td>\n",
       "      <td>2011</td>\n",
       "      <td>17.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1173.0</td>\n",
       "      <td>934.0</td>\n",
       "      <td>728.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>421010001001</td>\n",
       "      <td>2012</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1347.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>790.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>421010001001</td>\n",
       "      <td>2013</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1481.0</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>826.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>421010001001</td>\n",
       "      <td>2014</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1489.0</td>\n",
       "      <td>1126.0</td>\n",
       "      <td>823.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          GEOID  year_evictions  eviction-filings  evictions  low-flag  \\\n",
       "0  421010001001            2010              16.0       13.0         0   \n",
       "1  421010001001            2011              17.0        7.0         0   \n",
       "2  421010001001            2012              14.0        7.0         0   \n",
       "3  421010001001            2013              15.0        9.0         0   \n",
       "4  421010001001            2014              13.0        4.0         0   \n",
       "\n",
       "   imputed  subbed  evictions_t-1  evictions_t-2  evictions_t-5  ...   units  \\\n",
       "0        0       1            8.0           10.0            9.0  ...   220.0   \n",
       "1        0       1           13.0            8.0            3.0  ...  1173.0   \n",
       "2        0       1            7.0           13.0           13.0  ...  1347.0   \n",
       "3        0       1            7.0            7.0           10.0  ...  1481.0   \n",
       "4        0       1            9.0            7.0            8.0  ...  1489.0   \n",
       "\n",
       "   occupied_units  vacant_units  for_rent_units  num_af_am_alone  num_hisp  \\\n",
       "0           189.0          94.0             0.0              0.0       0.0   \n",
       "1           934.0         728.0            38.0             76.0      94.0   \n",
       "2          1024.0         790.0           124.0            104.0     111.0   \n",
       "3          1094.0         826.0           139.0             88.0     122.0   \n",
       "4          1126.0         823.0           155.0             89.0      57.0   \n",
       "\n",
       "  black_alone_owner_occupied num_with_high_school_degree num_with_ged  \\\n",
       "0                        0.0                         NaN          NaN   \n",
       "1                       14.0                         NaN          NaN   \n",
       "2                       12.0                         NaN          NaN   \n",
       "3                       12.0                        52.0          0.0   \n",
       "4                       12.0                        53.0          0.0   \n",
       "\n",
       "   num_unemployed  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             0.0  \n",
       "3             0.0  \n",
       "4             0.0  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/final_merged_df.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data By Year\n",
    "We have data for 2009 to 2016. We want to split this data into training set /\n",
    "test set pairs using a temporal cross-validation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_start</th>\n",
       "      <th>train_end</th>\n",
       "      <th>test_start</th>\n",
       "      <th>test_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010</td>\n",
       "      <td>2011</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>2011</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>2012</td>\n",
       "      <td>2013</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>2013</td>\n",
       "      <td>2014</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>2014</td>\n",
       "      <td>2015</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  train_start train_end test_start test_end\n",
       "0        2010      2010       2011     2011\n",
       "1        2010      2011       2012     2012\n",
       "2        2010      2012       2013     2013\n",
       "3        2010      2013       2014     2014\n",
       "4        2010      2014       2015     2015\n",
       "5        2010      2015       2016     2016"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = pipeline.split_by_year(df, colname='year_evictions')\n",
    "pipeline.split_boundaries(splits, colname='year_evictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "We want to clean each of our training set / test set pairs. We use a function\n",
    "called `clean_split()` that cleans both sets at once, making sure to clean the\n",
    "test data using the same bins and categories applied to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32mdef\u001b[0m \u001b[0mclean_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m    \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m    \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_overall_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m    \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_overall_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m    \u001b[0mfeatures_generator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_feature_generators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m    \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \\\n",
      "        \u001b[0mclean_and_create_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\n",
      "\u001b[1;33m\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%psource pipeline.clean_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sinclairtarget/harris/ml/philly-evictions/.venv/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/sinclairtarget/harris/ml/philly-evictions/.venv/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1076: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "cleaned_splits = [pipeline.clean_split(split) for split in splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Labeling\n",
    "We plan to use both regression-based models and binary classifiers. For our\n",
    "binary classifiers, we will need to label our data using a binary label.\n",
    "\n",
    "Our binary label separates block groups into two classes: \"high\" and \"low\"\n",
    "eviction rate block groups. The \"high\" eviction rate block groups are those\n",
    "that we believe should be prioritized for intervention.\n",
    "\n",
    "Any block group with more than 14 evictions is considered a \"high\" eviction\n",
    "rate block group. Roughly 16% of Philadelphia block groups are \"high\" eviction\n",
    "rate block groups. We have picked this lower boundary because we know that\n",
    "Philadelphia can afford to target about 16% of block groups for intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_splits = [pipeline.label(split, lower_bound=14, drop_column=True)\n",
    "                  for split in cleaned_splits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Model Generation\n",
    "### Binary Classifiers\n",
    "Our binary classifiers are given by the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                    random_state=1234, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " 'KNN': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                      weights='uniform'),\n",
       " 'DT': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                        max_features=None, max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, presort=False,\n",
       "                        random_state=1234, splitter='best'),\n",
       " 'SVM': SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "     decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "     kernel='linear', max_iter=-1, probability=True, random_state=1234,\n",
       "     shrinking=True, tol=0.001, verbose=False),\n",
       " 'RF': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                        max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
       "                        n_jobs=None, oob_score=False, random_state=1234,\n",
       "                        verbose=0, warm_start=False),\n",
       " 'GB': GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                            learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                            max_features=None, max_leaf_nodes=None,\n",
       "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                            n_iter_no_change=None, presort='auto',\n",
       "                            random_state=1234, subsample=1.0, tol=0.0001,\n",
       "                            validation_fraction=0.1, verbose=0,\n",
       "                            warm_start=False),\n",
       " 'AB': AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
       "                    n_estimators=50, random_state=1234),\n",
       " 'NB': GaussianNB(priors=None, var_smoothing=1e-09),\n",
       " 'ET': ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators='warn',\n",
       "                      n_jobs=None, oob_score=False, random_state=1234, verbose=0,\n",
       "                      warm_start=False),\n",
       " 'BC': BaggingClassifier(base_estimator=None, bootstrap=True, bootstrap_features=False,\n",
       "                   max_features=1.0, max_samples=1.0, n_estimators=10,\n",
       "                   n_jobs=None, oob_score=False, random_state=1234, verbose=0,\n",
       "                   warm_start=False)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We plan to run a grid search using the following hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LR': {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1]},\n",
       " 'KNN': {'n_neighbors': [5, 10],\n",
       "  'weights': ['uniform', 'distance'],\n",
       "  'algorithm': ['auto', 'ball_tree', 'kd_tree']},\n",
       " 'DT': {'criterion': ['gini', 'entropy'],\n",
       "  'max_depth': [5, 50],\n",
       "  'max_features': [None],\n",
       "  'min_samples_split': [5, 10]},\n",
       " 'SVM': {'C': [0.01, 0.1]},\n",
       " 'RF': {'n_estimators': [100, 1000],\n",
       "  'max_depth': [5, 50],\n",
       "  'max_features': ['sqrt', 'log2'],\n",
       "  'min_samples_split': [5, 10]},\n",
       " 'GB': {'n_estimators': [100, 1000],\n",
       "  'learning_rate': [0.01, 0.05],\n",
       "  'subsample': [0.1, 0.5],\n",
       "  'max_depth': [5, 10]},\n",
       " 'AB': {'algorithm': ['SAMME', 'SAMME.R'], 'n_estimators': [100, 1000]},\n",
       " 'NB': {},\n",
       " 'ET': {'n_estimators': [100, 1000],\n",
       "  'criterion': ['gini', 'entropy'],\n",
       "  'max_depth': [5, 10],\n",
       "  'max_features': ['sqrt', 'log2'],\n",
       "  'min_samples_split': [5, 10]},\n",
       " 'BC': {'n_estimators': [100, 1000]}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.clf_small_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We also want to evaluate our models at the following thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We run our models for each of our splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3d95d965422a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabeled_splits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     results = pipeline.run_clf_loop(\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mtest_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     15\u001b[0m     \u001b[0mresults_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/harris/ml/philly-evictions/pipeline/loop.py\u001b[0m in \u001b[0;36mrun_clf_loop\u001b[1;34m(test_df, train_df, clfs, grid, label_col, thresholds)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameter_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/harris/ml/philly-evictions/.venv/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1531\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[1;32m-> 1532\u001b[1;33m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[0;32m   1533\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1534\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/harris/ml/philly-evictions/.venv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    720\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~/harris/ml/philly-evictions/.venv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/harris/ml/philly-evictions/.venv/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[0;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'infinity'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'NaN, infinity'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "models = pipeline.clfs\n",
    "grid = pipeline.clf_small_grid\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'split',\n",
    "    'classifier',\n",
    "    'parameters',\n",
    "    'threshold',\n",
    "] + pipeline.evaluate.ClassifierEvaluator.metric_names())\n",
    "\n",
    "for i, (train_df, test_df) in enumerate(labeled_splits, start=1):\n",
    "    results = pipeline.run_clf_loop(\n",
    "        test_df, train_df, models, grid, 'label', thresholds\n",
    "    )\n",
    "    results_df.append([i] + results, ignore_index=True)\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Models\n",
    "Our regression models are given by the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.regs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We plan to run a grid search using the following hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.reg_small_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We run our models for each of our splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = pipeline.regs\n",
    "grid = pipeline.reg_small_grid\n",
    "\n",
    "results_df = pd.DataFrame(columns=[\n",
    "    'split',\n",
    "    'classifier',\n",
    "    'parameters',\n",
    "] + pipeline.evaluate.RegressionEvaluator.metric_names())\n",
    "\n",
    "for i, (train_df, test_df) in enumerate(cleaned_splits, start=1):\n",
    "    results = pipeline.run_reg_loop( test_df, train_df, models, grid, 'evictions')\n",
    "    results_df.append([i] + results, ignore_index=True)\n",
    "\n",
    "results_df.head()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,md",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
